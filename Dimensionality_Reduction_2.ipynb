{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. What is a projection and how is it used in PCA?**\n"
      ],
      "metadata": {
        "id": "eD_jtnCrmWV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Projection?\n",
        "\n",
        "In the context of mathematics and machine learning, a projection refers to the process of mapping a high-dimensional data point onto a lower-dimensional space. This is typically done by projecting the data onto a subspace defined by a set of basis vectors. The result is a representation of the original data that retains essential information while reducing complexity.\n",
        "\n",
        "How is Projection Used in PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a widely used dimensionality reduction technique that employs the concept of projection to transform high-dimensional data into a lower-dimensional space. Here's how projection is integral to PCA:\n",
        "\n",
        "1. Data Centering\n",
        "\n",
        "  Description: Before performing PCA, the data is centered by subtracting the mean of each feature. This step ensures that the principal components are calculated from a mean-centered dataset.\n",
        "\n",
        "2. Covariance Matrix Calculation\n",
        "\n",
        "  Description: PCA computes the covariance matrix of the centered data. The covariance matrix captures the relationships between features, indicating how they vary together.\n",
        "\n",
        "3. Eigenvalue and Eigenvector Calculation\n",
        "\n",
        "  Description: The eigenvalues and eigenvectors of the covariance matrix are calculated. The eigenvectors represent the directions of maximum variance (principal components), while the eigenvalues indicate the magnitude of variance along those directions.\n",
        "\n",
        "4. Selecting Principal Components\n",
        "\n",
        "  Description: The eigenvectors (principal components) are sorted in descending order based on their corresponding eigenvalues. The top k eigenvectors are selected to form a new basis for the lower-dimensional space, where k is the desired number of dimensions.\n",
        "\n",
        "5. Projection onto the New Subspace\n",
        "\n",
        "  Process:\n",
        "\n",
        "  The original data points are projected onto the selected principal components (eigenvectors). This is done by performing a linear transformation, which can be mathematically represented as:\n",
        "\n",
        "  Y=XW\n",
        "\n",
        "  Where:\n",
        "\n",
        "  Y is the matrix of projected data (reduced dimensions).X is the original mean-centered data matrix.W is the matrix of selected eigenvectors (principal components).\n",
        "\n",
        "  Result: The resulting matrix Y contains the coordinates of the original data points expressed in the new lower-dimensional space defined by the principal components.\n",
        "\n",
        "Visualization of Projection in PCA\n",
        "\n",
        "Original Space vs. Projected Space: In a visual sense, if you think of a 3D space where the data points lie, projecting them onto a plane (2D space) means finding the points on that plane that are closest to the original points. The projection retains the structure of the data while simplifying its representation.\n",
        "\n",
        "Importance of Projection in PCA\n",
        "\n",
        "Dimensionality Reduction: By projecting high-dimensional data onto a lower-dimensional space, PCA helps reduce the complexity of the dataset, making it easier to visualize and analyze.\n",
        "\n",
        "Data Compression: The projected data retains the most critical features (directions of maximum variance), allowing for efficient storage and processing.\n",
        "Noise Reduction: Since PCA focuses on the directions of highest variance, it often filters out noise and less significant features, which may contribute less to the overall structure of the data."
      ],
      "metadata": {
        "id": "tA8JdNQ_ylFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. How does the optimization problem in PCA work, and what is it trying to achieve?**\n"
      ],
      "metadata": {
        "id": "hnsAfHo1mgY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization problem in Principal Component Analysis (PCA) revolves around finding a new set of orthogonal axes (principal components) that capture the maximum variance in the data while reducing its dimensionality. The goal is to project the original high-dimensional data into a lower-dimensional space while preserving as much information as possible. Here’s a detailed breakdown of how this optimization problem works and what it aims to achieve:\n",
        "\n",
        "Goal of PCA\n",
        "\n",
        "The primary goal of PCA is to reduce the dimensionality of a dataset while retaining as much variability (information) as possible. This is achieved by identifying the directions (principal components) along which the data varies the most.\n",
        "\n",
        "Objective of the Optimization Problem\n",
        "\n",
        "The optimization problem in PCA aims to:\n",
        "\n",
        "Maximize Variance: Find directions in the data that maximize variance, which indicates that the directions capture the most information about the data distribution.\n",
        "\n",
        "Minimize Reconstruction Error: Although not explicitly stated as a constraint, projecting onto principal components leads to a lower reconstruction error when attempting to reconstruct the original data from the reduced dimensions. The first few components capture most of the variance, leading to better approximations of the original data.\n",
        "\n",
        "Simplify the Dataset: By reducing the number of dimensions while retaining the most significant variance, PCA simplifies the dataset, making it easier to visualize, analyze, and use in machine learning models."
      ],
      "metadata": {
        "id": "d6Vqvg0szEfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. What is the relationship between covariance matrices and PCA?**\n"
      ],
      "metadata": {
        "id": "1KTzx2CLmgWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA and Covariance Matrices\n",
        "\n",
        "1. Maximizing Variance\n",
        "\n",
        "  The primary goal of PCA is to find directions in the feature space (principal components) that maximize the variance of the data. The covariance matrix plays a critical role in this process:\n",
        "\n",
        "  Variance and Covariance: The diagonal elements of the covariance matrix represent the variances of each feature, while the off-diagonal elements represent the covariances between features. By analyzing the covariance matrix, PCA can identify which combinations of features contribute the most to the overall variance.\n",
        "\n",
        "2. Eigenvalue Decomposition\n",
        "\n",
        "  PCA relies on eigenvalue decomposition of the covariance matrix:\n",
        "\n",
        "  Eigenvalues and Eigenvectors: When PCA is applied, the covariance matrix Σ is decomposed into its eigenvalues and eigenvectors:\n",
        "\n",
        "  Σv=λv\n",
        "\n",
        "  Where:\n",
        "\n",
        "  v represents the eigenvector (principal component direction).\n",
        "\n",
        "  λ represents the corresponding eigenvalue (variance along that direction).\n",
        "\n",
        "3. Principal Components\n",
        "\n",
        "  The eigenvectors of the covariance matrix indicate the directions of maximum variance in the data. The eigenvalues provide a measure of the variance captured by each principal component.\n",
        "\n",
        "  By selecting the top k eigenvectors corresponding to the largest k eigenvalues, PCA effectively reduces the dimensionality of the data while retaining the most important variance. The principal components are then obtained by projecting the original data onto these selected eigenvectors."
      ],
      "metadata": {
        "id": "URXDMAJI0JJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. How does the choice of number of principal components impact the performance of PCA?**\n"
      ],
      "metadata": {
        "id": "TJzWT4GjmgTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the number of principal components to retain in Principal Component Analysis (PCA) significantly impacts the performance of the resulting model or analysis. This choice affects several aspects, including variance retention, model interpretability, and the risk of overfitting or underfitting. Here’s a detailed breakdown of how the number of principal components influences PCA performance:\n",
        "\n",
        "1. Variance Retention\n",
        "\n",
        "  Information Capture: Each principal component captures a certain amount of variance from the original dataset. The more components you retain, the more information about the data structure is preserved.\n",
        "\n",
        "  Explained Variance Ratio: By examining the explained variance ratio for each principal component, you can assess how much variance each component contributes relative to the total variance. Retaining too few components might lead to significant loss of information, which can degrade model performance.\n",
        "\n",
        "  Example: If you retain only the first principal component, you capture the most significant variance direction, but you may lose other important variations captured by subsequent components.\n",
        "\n",
        "2. Dimensionality Reduction and Noise Filtering\n",
        "\n",
        "  Noise Reduction: PCA helps reduce noise in the dataset by focusing on the directions of maximum variance. However, if too many components are retained, you might include components that capture noise rather than meaningful signal.\n",
        "\n",
        "  Balancing Act: There is a trade-off between reducing dimensionality (to improve performance) and retaining enough components to capture the essential structure of the data. Finding the right number of components is crucial to filter out noise without discarding important information.\n",
        "\n",
        "3. Model Complexity and Overfitting\n",
        "\n",
        "  Overfitting Risk: Retaining too many principal components can lead to overfitting, especially in machine learning tasks. Overfitting occurs when the model learns noise and random fluctuations in the training data, resulting in poor generalization to unseen data.\n",
        "\n",
        "  Simpler Models: Reducing the number of components can lead to a simpler model that captures the underlying structure more robustly, improving generalization performance.\n",
        "\n",
        "4. Computational Efficiency\n",
        "\n",
        "  Efficiency: Reducing the number of principal components also reduces the complexity of subsequent computations. Models built on a lower-dimensional representation are often faster to train and require less memory.\n",
        "\n",
        "  Scalability: In large datasets, retaining a manageable number of components can make PCA and subsequent modeling techniques more scalable and efficient.\n",
        "\n",
        "5. Interpretability\n",
        "\n",
        "  Interpretability of Results: Retaining fewer principal components often makes the results more interpretable. With a smaller number of components, it is easier to understand the underlying patterns and the influence of each component.\n",
        "\n",
        "  Feature Importance: Fewer components can lead to clearer insights about which features contribute most significantly to the variance in the data, aiding in feature selection and understanding data relationships.\n",
        "\n",
        "6. Visualizing Data\n",
        "\n",
        "  Visualization: When visualizing data (especially for exploratory data analysis), reducing to 2 or 3 principal components allows for effective graphical representation. This can help identify clusters, trends, and outliers in the data.\n",
        "\n",
        "  Diminishing Returns: Beyond a certain point, adding more components may not provide significantly better insights or visualization, as the added dimensions may represent very little variance."
      ],
      "metadata": {
        "id": "X92LtJv70h0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?**\n"
      ],
      "metadata": {
        "id": "T46UOo7-mgRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is primarily a dimensionality reduction technique, but it can also be effectively used for feature selection. While traditional feature selection methods identify and retain original features based on their importance or contribution to the target variable, PCA transforms the original features into a new set of uncorrelated variables called principal components. Here’s how PCA can be used for feature selection and its associated benefits:\n",
        "\n",
        "How PCA Can Be Used in Feature Selection\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Before applying PCA, the dataset is typically standardized or normalized, especially if the features are on different scales. This ensures that PCA captures the true relationships among features without being biased by their scales.\n",
        "\n",
        "Applying PCA:\n",
        "\n",
        "PCA is applied to the standardized dataset to derive the principal components. These components are linear combinations of the original features and are ordered by the amount of variance they explain.\n",
        "\n",
        "Explained Variance Analysis:\n",
        "\n",
        "After applying PCA, each principal component is associated with an explained variance ratio, which indicates how much of the total variance in the dataset is captured by that component.\n",
        "\n",
        "By examining the cumulative explained variance, one can determine how many components are needed to capture a sufficient amount of the total variance (often, a threshold like 90% is used).\n",
        "\n",
        "Selecting Principal Components:\n",
        "\n",
        "Once the number of components to retain is determined, these components can be used as new features in place of the original features. This effectively reduces the dimensionality of the dataset while still capturing the majority of the information.\n",
        "\n",
        "Alternatively, you can analyze the loading of the original features on the selected principal components to identify which original features contribute most to those components. Features with high absolute loading values on retained components can be considered significant and retained.\n",
        "\n",
        "Using Selected Features for Modeling:\n",
        "\n",
        "The reduced dataset (with the selected principal components) can then be used to train machine learning models, improving efficiency and potentially enhancing model performance due to reduced noise and less overfitting.\n",
        "Benefits of Using PCA for Feature Selection\n",
        "\n",
        "Dimensionality Reduction:\n",
        "\n",
        "PCA helps reduce the number of features while retaining most of the data's variability, which can improve computational efficiency and reduce the risk of overfitting.\n",
        "\n",
        "Noise Reduction:\n",
        "\n",
        "By focusing on the components that capture the most variance, PCA can help filter out noise and less significant features, resulting in a cleaner dataset for modeling.\n",
        "\n",
        "Dealing with Multicollinearity:\n",
        "\n",
        "PCA addresses multicollinearity (high correlations among features) by transforming correlated features into uncorrelated principal components. This can lead to more stable and interpretable models.\n",
        "\n",
        "Improved Model Performance:\n",
        "\n",
        "By retaining only the most informative features (principal components), PCA can improve model performance, especially in cases with a large number of features compared to the number of observations.\n",
        "\n",
        "Visualization:\n",
        "\n",
        "PCA can facilitate visualization by reducing high-dimensional data to two or three dimensions, allowing for better insights into the data structure and potential relationships.\n",
        "\n",
        "Automated Feature Selection:\n",
        "\n",
        "PCA provides an automated way to select features based on variance, reducing the reliance on subjective feature selection methods."
      ],
      "metadata": {
        "id": "W9sGtgBS02F0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. What are some common applications of PCA in data science and machine learning?**\n"
      ],
      "metadata": {
        "id": "UWbaS9x5mmOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a versatile technique widely used in data science and machine learning for various applications. Here are some common applications of PCA:\n",
        "\n",
        "1. Dimensionality Reduction\n",
        "\n",
        "  High-Dimensional Data: PCA is commonly applied to datasets with many features (dimensions), such as image data or gene expression data. It reduces the number of features while preserving the most important information, making subsequent analyses more manageable and efficient.\n",
        "\n",
        "  Visualization: By projecting high-dimensional data into 2D or 3D space, PCA facilitates visualization, allowing data scientists to explore patterns, clusters, and relationships in the data.\n",
        "\n",
        "2. Data Preprocessing\n",
        "\n",
        "  Noise Reduction: PCA helps filter out noise by focusing on the principal components that capture the most variance, which can improve the performance of machine learning models by reducing overfitting.\n",
        "\n",
        "  Feature Engineering: PCA can be used to create new features (principal components) from existing features, providing a potentially more informative representation of the data for modeling.\n",
        "\n",
        "3. Image Compression\n",
        "\n",
        "  Image Processing: PCA is used for image compression by reducing the dimensionality of image data while retaining essential features. The principal components represent the most significant information in the images, allowing for efficient storage and transmission.\n",
        "\n",
        "  Face Recognition: Techniques like Eigenfaces utilize PCA for face recognition, where the principal components represent significant facial features that differentiate individuals.\n",
        "\n",
        "4. Genomics and Bioinformatics\n",
        "\n",
        "  Gene Expression Analysis: In bioinformatics, PCA is used to analyze high-dimensional gene expression data. It helps identify patterns of gene activity, detect outliers, and reduce the complexity of the dataset for further analysis.\n",
        "\n",
        "  Microbiome Data: PCA can help visualize and interpret complex microbiome datasets, revealing relationships and clusters among different microbial communities.\n",
        "\n",
        "5. Market Research and Customer Segmentation\n",
        "\n",
        "  Customer Behavior Analysis: PCA helps analyze customer data by reducing the number of variables and identifying key factors that influence purchasing behavior, enabling targeted marketing strategies.\n",
        "\n",
        "  Product Development: Businesses can use PCA to identify trends in product preferences and features, aiding in the design and development of new products.\n",
        "\n",
        "6. Finance and Risk Management\n",
        "\n",
        "  Portfolio Management: In finance, PCA is employed to analyze the risks and returns of investment portfolios. It helps identify underlying factors that drive asset prices and reduce dimensionality in financial datasets.\n",
        "\n",
        "  Credit Risk Analysis: PCA can be used to identify key variables influencing credit risk, simplifying the analysis and improving decision-making processes.\n",
        "\n",
        "7. Speech and Audio Processing\n",
        "\n",
        "  Speech Recognition: PCA is utilized in speech processing to reduce the dimensionality of audio features, helping to improve the performance of speech recognition systems.\n",
        "\n",
        "  Audio Classification: PCA can be applied to classify different audio signals by reducing the feature space and highlighting significant patterns.\n",
        "\n",
        "8. Natural Language Processing (NLP)\n",
        "\n",
        "  Topic Modeling: In NLP, PCA can help reduce the dimensionality of word vectors or document-term matrices, making it easier to identify underlying topics or themes in text data.\n",
        "\n",
        "  Text Classification: PCA can preprocess text data, aiding in classification tasks by highlighting the most significant features while reducing noise."
      ],
      "metadata": {
        "id": "IyfFNGiO1FL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q7.What is the relationship between spread and variance in PCA?**\n"
      ],
      "metadata": {
        "id": "HjttdhPNmmL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relationship Between Spread and Variance in PCA\n",
        "\n",
        "PCA’s Goal: The primary goal of PCA is to find the directions (principal components) in which the data has the greatest variance, effectively capturing the spread of the data in a lower-dimensional space.\n",
        "\n",
        "Variance as a Measure of Spread: In the context of PCA:\n",
        "\n",
        "Variance can be considered a numerical measure of the spread of data points along different dimensions. PCA identifies the axes (principal components) where this spread is maximized.\n",
        "\n",
        "Each principal component corresponds to a direction in the feature space, and the variance explained by each component reflects how much spread exists along that direction.\n",
        "\n",
        "Eigenvalues and Variance: When PCA is applied, it involves computing the covariance matrix of the dataset and then performing eigenvalue decomposition:\n",
        "\n",
        "The eigenvalues obtained represent the variance captured by each principal component. Larger eigenvalues indicate directions with greater spread (variance), while smaller eigenvalues correspond to directions with less spread.\n",
        "The principal components are ranked by their associated eigenvalues, allowing PCA to focus on the components that capture the most variance (spread) first."
      ],
      "metadata": {
        "id": "w8EL1_hl1haS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q8. How does PCA use the spread and variance of the data to identify principal components?**\n"
      ],
      "metadata": {
        "id": "yxVwDGFhmw0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps in PCA Using Spread and Variance\n",
        "\n",
        "Step 1: Data Standardization\n",
        "\n",
        "Preprocessing: Before applying PCA, the data is usually standardized (mean-centered and scaled) to ensure that all features contribute equally. This step involves subtracting the mean from each feature and dividing by the standard deviation.\n",
        "\n",
        "Importance of Standardization: Standardization is crucial because PCA is sensitive to the scale of the features. Features with larger ranges could dominate the variance calculations, leading to misleading results.\n",
        "\n",
        "Step 2: Compute the Covariance Matrix\n",
        "\n",
        "Covariance Matrix: The covariance matrix is calculated to capture how the dimensions of the data vary with respect to each other. The covariance between two features indicates the degree to which they change together.\n",
        "\n",
        "Spread in Covariance: The covariance matrix summarizes the spread of the data in multiple dimensions, capturing the relationships between different features.\n",
        "\n",
        "Step 3: Eigenvalue Decomposition\n",
        "\n",
        "Eigenvalues and Eigenvectors: PCA performs eigenvalue decomposition on the covariance matrix to obtain eigenvalues and eigenvectors. The eigenvectors represent the directions (principal components), while the eigenvalues indicate the amount of variance (spread) captured by each principal component.\n",
        "\n",
        "Interpretation of Eigenvalues:\n",
        "\n",
        "A larger eigenvalue means that the corresponding eigenvector (principal component) captures more variance from the data, indicating that it is a direction with significant spread.\n",
        "\n",
        "Conversely, smaller eigenvalues correspond to directions with less variance, indicating less spread and often less informative dimensions.\n",
        "\n",
        "Step 4: Selecting Principal Components\n",
        "\n",
        "Ranking Principal Components: The principal components are ranked based on their associated eigenvalues. The first principal component (largest eigenvalue) captures the direction of maximum variance, followed by the second principal component, and so on.\n",
        "\n",
        "Cumulative Explained Variance: A cumulative explained variance plot can help determine how many principal components to retain. Typically, a threshold (e.g., 90% of the total variance) is set to decide how many components to keep.\n",
        "\n",
        "\n",
        "Step 5: Transforming the Data\n",
        "\n",
        "Projecting Data onto Principal Components: The original data can be projected onto the selected principal components, resulting in a lower-dimensional representation that retains the most critical information (spread) about the data.\n",
        "\n",
        "\n",
        "Z=X⋅W\n",
        "\n",
        "Where\n",
        "Z is the transformed data, X is the original data, and W is the matrix of selected principal components."
      ],
      "metadata": {
        "id": "ATZr1gBi1xNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q9. How does PCA handle data with high variance in some dimensions but low variance in others?**"
      ],
      "metadata": {
        "id": "40FT7uU4mySl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is particularly effective in handling datasets with varying levels of variance across different dimensions. Here's how PCA addresses this scenario:\n",
        "\n",
        "1. Understanding High and Low Variance Dimensions\n",
        "\n",
        "  High Variance Dimensions: These are dimensions (features) where the data points are widely spread out. High variance often indicates that these features contain significant information that can be useful for distinguishing between different data points or classes.\n",
        "\n",
        "  Low Variance Dimensions: In contrast, low variance dimensions have data points that are closely clustered around the mean. These dimensions often carry less informative value and may contain noise rather than useful signal.\n",
        "\n",
        "2. Role of Covariance Matrix\n",
        "\n",
        "  Covariance Calculation: PCA starts by calculating the covariance matrix of the dataset, which captures how the features (dimensions) vary together. The diagonal elements of the covariance matrix represent the variance of each feature, and off-diagonal elements represent the covariance between different features.\n",
        "\n",
        "  Identifying Variance: The covariance matrix provides insight into which dimensions have high variance and which have low variance. High variance features will have larger diagonal entries in the covariance matrix.\n",
        "\n",
        "3. Eigenvalue Decomposition\n",
        "\n",
        "  Eigenvalues and Eigenvectors: After calculating the covariance matrix, PCA performs eigenvalue decomposition to obtain eigenvalues and eigenvectors:\n",
        "  Eigenvalues: Each eigenvalue corresponds to the amount of variance explained by its associated eigenvector (principal component). High eigenvalues indicate directions in which data has high variance (spread).\n",
        "\n",
        "  Eigenvectors: The eigenvectors are the directions (principal components) that maximize the variance in the dataset.\n",
        "\n",
        "4. Selecting Principal Components\n",
        "\n",
        "  Ranking Principal Components: Principal components are ranked by their associated eigenvalues. The first principal component has the largest eigenvalue, capturing the direction with the maximum variance. Subsequent components capture increasingly lower amounts of variance.\n",
        "\n",
        "  Filtering Low Variance Dimensions: PCA inherently filters out dimensions with low variance by focusing on the components that explain the most variance. Low variance dimensions tend to have small eigenvalues, and these components can be discarded based on a variance threshold (e.g., retaining components that cumulatively explain 90% of the variance).\n",
        "\n",
        "5. Dimensionality Reduction\n",
        "\n",
        "  Projection onto Principal Components: By projecting the original high-dimensional data onto the selected principal components (those with high variance), PCA effectively reduces the dimensionality of the data while preserving the most significant patterns.\n",
        "\n",
        "  Effect on Performance: This dimensionality reduction improves computational efficiency, reduces the risk of overfitting, and enhances model performance, particularly in machine learning tasks where noise and less informative dimensions may lead to poorer results.\n",
        "\n",
        "6. Handling High-Dimensional Spaces\n",
        "\n",
        "  Curse of Dimensionality: In high-dimensional spaces, the differences in variance across dimensions can lead to issues like the curse of dimensionality, where models struggle to generalize due to sparse data. PCA mitigates this by reducing the dimensions to those that hold the most relevant information.\n",
        "\n",
        "  Feature Interactions: By focusing on principal components, PCA captures interactions among features that might not be apparent when looking at individual features, leading to a better understanding of the underlying data structure."
      ],
      "metadata": {
        "id": "0wiGaSFn2JSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuNWPlYumfuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
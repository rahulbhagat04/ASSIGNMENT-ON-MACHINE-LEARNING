{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.**\n"
      ],
      "metadata": {
        "id": "zIlCL0qyoLCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues and eigenvectors are fundamental concepts in linear algebra, particularly in the context of matrix operations and transformations. They play a crucial role in various applications, including Principal Component Analysis (PCA) and many other areas in mathematics, physics, and engineering.\n",
        "\n",
        "1. Definitions\n",
        "\n",
        "Eigenvalues: An eigenvalue is a scalar that indicates how much the eigenvector is stretched or compressed during a linear transformation represented by a matrix. Mathematically, if A is a square matrix and v is a non-zero vector (the eigenvector), then the eigenvalue Œª satisfies the equation:\n",
        "\n",
        "Av=Œªv\n",
        "\n",
        "Here,\n",
        "\n",
        "Av produces a new vector that is a scaled version of v.\n",
        "\n",
        "Eigenvectors: An eigenvector is a non-zero vector that, when transformed by a matrix A, only changes in scale (not direction). Each eigenvector is associated with an eigenvalue. The eigenvector must satisfy the equation above.\n",
        "\n",
        "2. Eigen-Decomposition\n",
        "\n",
        "Eigen-decomposition is the process of decomposing a square matrix A into its eigenvalues and eigenvectors. It can be represented mathematically as:\n",
        "\n",
        "A=VŒõV‚àí1\n",
        "\n",
        "Where:\n",
        "\n",
        "A is the original matrix.\n",
        "\n",
        "V is a matrix whose columns are the eigenvectors of A.\n",
        "\n",
        "Œõ is a diagonal matrix containing the eigenvalues of A along its diagonal.V‚àí1 is the inverse of the matrix V.\n",
        "\n"
      ],
      "metadata": {
        "id": "qczc1B-a3Cwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. What is eigen decomposition and what is its significance in linear algebra?**\n"
      ],
      "metadata": {
        "id": "kCQ8iVWsorcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigen-decomposition (or eigendecomposition) is a matrix factorization technique in linear algebra where a square matrix A is decomposed into a product involving its eigenvalues and eigenvectors. This decomposition is particularly useful in understanding the transformations represented by matrices, as it allows us to express the matrix in terms of its fundamental components.\n",
        "\n",
        "1. Definition of Eigen-Decomposition\n",
        "\n",
        "For a square matrix A, if it can be decomposed, it can be written as:\n",
        "\n",
        "A=VŒõV‚àí1\n",
        "\n",
        "Where:\n",
        "\n",
        "A is the original matrix.\n",
        "\n",
        "V is a matrix whose columns are the eigenvectors of A.\n",
        "\n",
        "Œõ is a diagonal matrix containing the eigenvalues of A along its diagonal.V‚àí1 is the inverse of the matrix V.\n",
        "\n",
        "Eigen-decomposition expresses\n",
        "ùê¥\n",
        "A in a way that separates its action (transformation) into two parts: scaling along the directions specified by the eigenvectors (contained in\n",
        "ùëâ\n",
        "V) and the amount of scaling specified by the eigenvalues (contained in\n",
        "Œõ\n",
        "Œõ).\n",
        "\n",
        "2. Significance of Eigen-Decomposition\n",
        "\n",
        "Eigen-decomposition provides insights into the properties and behaviors of matrices, which is essential in many areas of linear algebra, applied mathematics, and data science. Here‚Äôs why it is significant:\n",
        "\n",
        "a) Understanding Linear Transformations\n",
        "\n",
        "Eigenvectors define the directions in which a transformation acts by stretching or compressing the data.\n",
        "\n",
        "Eigenvalues tell us the factor by which the data is stretched or compressed along each eigenvector direction.\n",
        "\n",
        "b) Simplifying Matrix Operations\n",
        "\n",
        "Power of a Matrix: Raising a matrix to a power is simplified by eigen-decomposition, as it becomes an operation on the eigenvalues (diagonal elements of Œõ) rather than on the entire matrix. For example:\n",
        "\n",
        "Ak=VŒõkV‚àí1\n",
        "\n",
        "This is useful in applications like Markov chains, where repeated transformations are applied.\n",
        "\n",
        "c) Applications in Principal Component Analysis (PCA)\n",
        "\n",
        "In PCA, eigen-decomposition helps identify the principal components by decomposing the covariance matrix of the data. The eigenvectors represent directions of maximum variance (principal components), and the eigenvalues indicate the amount of variance along each direction.\n",
        "\n",
        "This enables dimensionality reduction by selecting the top eigenvalues and corresponding eigenvectors, reducing data while retaining the most important features.\n",
        "\n",
        "d) Solving Systems of Differential Equations\n",
        "\n",
        "In systems of linear differential equations, eigen-decomposition enables the solution by transforming the system into simpler, decoupled equations involving eigenvalues.\n",
        "\n",
        "e) Matrix Diagonalization\n",
        "\n",
        "Diagonal matrices are easier to work with in calculations, and eigen-decomposition allows certain matrices to be represented in diagonal form. Diagonalizing a matrix can simplify many operations and lead to efficient computations, particularly in complex algorithms.\n",
        "\n",
        "f) Explaining Variance in Data Analysis\n",
        "\n",
        "In data science, eigenvalues are used to explain the variance in datasets, as seen in PCA. The eigenvectors associated with the largest eigenvalues capture the most significant patterns, trends, or features in the data."
      ],
      "metadata": {
        "id": "gIew06sfrXmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.**\n"
      ],
      "metadata": {
        "id": "2o8MYsulorZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a square matrix\n",
        "\n",
        "A to be diagonalizable using the eigen-decomposition approach, it must meet specific conditions. These conditions relate to the matrix‚Äôs eigenvalues and eigenvectors.\n",
        "\n",
        "Conditions for Diagonalizability\n",
        "\n",
        "A square matrix A of order n√ón is diagonalizable if and only if:\n",
        "\n",
        "It has n linearly independent eigenvectors. This means the eigenvectors must span the entire n-dimensional vector space.\n",
        "\n",
        "The matrix has distinct eigenvalues, or, if eigenvalues are repeated, their geometric multiplicities (number of independent eigenvectors corresponding to each eigenvalue) equal their algebraic multiplicities (the number of times each eigenvalue appears in the characteristic polynomial).\n",
        "\n",
        "Explanation\n",
        "\n",
        "These conditions ensure that the matrix can be transformed (or \"conjugated\") into a diagonal matrix using a similarity transformation. For any matrix A, if it has a full set of linearly independent eigenvectors, then it can be decomposed as:\n",
        "\n",
        "A=VŒõV‚àí1\n",
        "\n",
        "where:\n",
        "\n",
        "V is an invertible matrix with the eigenvectors of A as its columns.Œõ is a diagonal matrix with eigenvalues of A along its diagonal.\n",
        "\n",
        "Proof Outline\n",
        "\n",
        "Step 1: Definition of Diagonalizability\n",
        "\n",
        "A matrix A is diagonalizable if there exists an invertible matrix V and a diagonal matrix Œõ such that:\n",
        "\n",
        "A=VŒõV‚àí1\n",
        "\n",
        "Multiplying both sides by V on the right, we get:\n",
        "\n",
        "AV=VŒõ\n",
        "\n",
        "This means that each column of V (each eigenvector) is mapped to a scalar multiple of itself by A, where each scalar multiple is given by the corresponding diagonal entry (eigenvalue) in Œõ.\n",
        "\n",
        "Step 2: Eigenvectors and Linear Independence\n",
        "\n",
        "For A to be diagonalizable, the matrix V (whose columns are eigenvectors of A) must be invertible. This happens if and only if the eigenvectors of A are linearly independent.\n",
        "\n",
        "Step 3: Relation Between Eigenvalues and Linear Independence of Eigenvectors\n",
        "Distinct Eigenvalues: If A has n distinct eigenvalues, then it automatically has n linearly independent eigenvectors, which guarantees diagonalizability.\n",
        "\n",
        "Repeated Eigenvalues: If eigenvalues are repeated, we must check the geometric multiplicity. Each eigenvalue‚Äôs geometric multiplicity (the number of independent eigenvectors corresponding to it) must equal its algebraic multiplicity for the matrix to be diagonalizable. If this holds, the matrix A will have n linearly independent eigenvectors, and thus it can be diagonalized."
      ],
      "metadata": {
        "id": "pQuXtU2hsddJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.**\n"
      ],
      "metadata": {
        "id": "u1RfU2gdorXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significance in the Context of Eigen-Decomposition\n",
        "\n",
        "The spectral theorem has several key implications for the eigen-decomposition of matrices:\n",
        "\n",
        "Guaranteed Diagonalization with Orthogonal Matrices: For symmetric matrices, the decomposition A=QŒõQT is always possible, which means symmetric matrices can be transformed into a diagonal form using orthogonal matrices. This is particularly useful because orthogonal transformations preserve the length and angles of vectors, making computations more stable and interpretable.\n",
        "\n",
        "Real Eigenvalues: For symmetric matrices, all eigenvalues are guaranteed to be real. This is beneficial in fields such as physics and engineering, where complex eigenvalues would be more challenging to interpret.\n",
        "\n",
        "Basis of Orthogonal Eigenvectors: The eigenvectors of symmetric matrices are not only linearly independent but also orthogonal to each other. This allows the matrix to be represented in an orthogonal basis, simplifying many calculations and interpretations, especially in Principal Component Analysis (PCA).\n",
        "\n",
        "3. Relation to Diagonalizability\n",
        "\n",
        "The spectral theorem implies that all symmetric matrices are diagonalizable, meaning they can always be decomposed into a form involving their eigenvalues and eigenvectors. For matrices that are not symmetric, diagonalizability is not guaranteed, and additional conditions (like having n linearly independent eigenvectors) are required."
      ],
      "metadata": {
        "id": "hCM_cJIDtT7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. How do you find the eigenvalues of a matrix and what do they represent?**\n"
      ],
      "metadata": {
        "id": "5iQP9zoNorUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the eigenvalues of a matrix, we solve a characteristic equation based on the matrix, which reveals values that represent how the matrix scales or transforms its eigenvectors. Here‚Äôs a step-by-step guide:\n",
        "\n",
        "1. Eigenvalue Definition\n",
        "\n",
        "For a given square matrix A, an eigenvalue Œª and its corresponding eigenvector v satisfy the equation:\n",
        "\n",
        "Av=Œªv\n",
        "\n",
        "This equation means that when the matrix A acts on v, it only scales v by the factor Œª, rather than changing its direction.\n",
        "\n",
        "2. Steps to Find Eigenvalues\n",
        "To find eigenvalues, follow these steps:\n",
        "\n",
        "Formulate the Characteristic Equation:\n",
        "\n",
        "Rewrite the equation\n",
        "\n",
        "Av=Œªv as\n",
        "\n",
        "(A‚àíŒªI)v=0, where\n",
        "\n",
        "I is the identity matrix.\n",
        "This equation states that v lies in the null space of\n",
        "\n",
        "(A‚àíŒªI), meaning det(A‚àíŒªI)=0 must hold for nontrivial solutions of v to exist.\n",
        "\n",
        "Solve the Characteristic Polynomial:\n",
        "\n",
        "Expand\n",
        "\n",
        "det(A‚àíŒªI)=0 to obtain a polynomial equation in Œª, known as the characteristic polynomial.\n",
        "Solve this polynomial to find the eigenvalues Œª of A.\n",
        "\n",
        "\n",
        "3. What Eigenvalues Represent\n",
        "\n",
        "Eigenvalues reveal key properties of the matrix‚Äôs transformation. They indicate:\n",
        "\n",
        "Scaling Effect: Each eigenvalue Œª shows the scaling factor applied to its corresponding eigenvector v under the transformation by A.\n",
        "Matrix Behavior:\n",
        "\n",
        "If ‚à£Œª‚à£>1: The transformation stretches vectors along the direction of v.\n",
        "\n",
        "If 0<‚à£Œª‚à£<1: It compresses vectors in that direction.\n",
        "\n",
        "If Œª=0: The matrix A compresses v to zero, suggesting a loss of dimension in that direction.\n",
        "\n",
        "Direction of Transformation: If eigenvalues are real and positive, the transformation maintains direction; if they‚Äôre negative, it flips directions along the eigenvectors.\n",
        "\n",
        "Stability in Dynamic Systems: In systems like differential equations, eigenvalues can indicate stability; negative real parts generally imply stability, while positive parts suggest instability."
      ],
      "metadata": {
        "id": "LJmVKPp5trSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. What are eigenvectors and how are they related to eigenvalues?**\n"
      ],
      "metadata": {
        "id": "0a8EPKsrorSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvectors are special vectors associated with a matrix that remain on the same line (or direction) after a transformation by the matrix. When a matrix A acts on an eigenvector v, the result is a scalar multiple of that eigenvector, rather than a new vector in a different direction. This scalar multiple is called the eigenvalue, denoted Œª."
      ],
      "metadata": {
        "id": "1lPrrFW2unJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?**\n"
      ],
      "metadata": {
        "id": "-wihceP3orPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Eigenvectors and eigenvalues have a clear geometric interpretation: they describe how a linear transformation, represented by a matrix, scales and rotates points in space along specific directions. This geometric perspective can be very helpful in understanding matrix transformations.\n",
        "\n",
        "1. Eigenvectors: Directions of Transformation\n",
        "\n",
        "An eigenvector of a matrix A is a vector that does not change direction under the transformation represented by A. It may get scaled (stretched or compressed) or flipped (if scaled negatively), but it stays on the same line.\n",
        "Geometrically, eigenvectors point in the directions where the matrix acts by pure scaling, without changing the direction. These directions are ‚Äúinvariant‚Äù under the transformation.\n",
        "\n",
        "2. Eigenvalues: Scaling Factors\n",
        "\n",
        "An eigenvalue corresponding to an eigenvector tells us how much the transformation scales that eigenvector.\n",
        "\n",
        "If the eigenvalue Œª is positive and greater than 1, it stretches the eigenvector in its direction.\n",
        "\n",
        "If 0<Œª<1, it compresses the eigenvector.\n",
        "\n",
        "If Œª=‚àí1, it flips the direction of the eigenvector.\n",
        "\n",
        "If Œª=0, it collapses the vector to a point at the origin, indicating that dimension is lost.\n",
        "\n",
        "3. Geometric Interpretation in 2D\n",
        "\n",
        "Imagine a transformation matrix A applied to a two-dimensional vector space, such as a rotation or scaling transformation. If we find two eigenvectors for A, they will generally be two specific directions along which the transformation only scales, without rotating."
      ],
      "metadata": {
        "id": "Ov0atG3Mu10j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q8. What are some real-world applications of eigen decomposition?**\n"
      ],
      "metadata": {
        "id": "hytgyVAeorH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigen decomposition has numerous applications across various fields due to its ability to reveal important structural information within matrices, making it a powerful tool in simplifying complex systems, reducing dimensions, and analyzing transformations. Here are some notable real-world applications of eigen decomposition:\n",
        "\n",
        "1. Principal Component Analysis (PCA)\n",
        "\n",
        "Application: Dimensionality reduction, data compression, and noise reduction in data analysis.\n",
        "\n",
        "How Eigen Decomposition is Used: PCA decomposes the covariance matrix of data into its eigenvalues and eigenvectors. The eigenvectors (principal components) with the largest eigenvalues represent the directions of greatest variance in the data, allowing a reduced-dimensional representation.\n",
        "\n",
        "Real-World Examples: Image compression, facial recognition, and exploratory data analysis in fields like finance, biology, and social sciences.\n",
        "\n",
        "2. Google PageRank Algorithm\n",
        "\n",
        "Application: Ranking web pages by relevance in search engines.\n",
        "\n",
        "How Eigen Decomposition is Used: Google‚Äôs PageRank algorithm models the internet as a graph where each page is a node, and hyperlinks are edges. The ranking matrix for web pages is computed using eigen decomposition. The largest eigenvalue and corresponding eigenvector of this matrix provide a steady-state distribution, determining the ‚Äúrank‚Äù of each webpage.\n",
        "\n",
        "Real-World Examples: PageRank is used in various recommendation systems and network analysis applications beyond web search, such as ranking scientific papers or products.\n",
        "\n",
        "3. Mechanical Vibrations and Stability Analysis\n",
        "\n",
        "Application: Analysis of physical systems such as buildings, bridges, or mechanical components to determine vibration modes and system stability.\n",
        "\n",
        "How Eigen Decomposition is Used: In structural engineering, eigenvalues and eigenvectors of the system‚Äôs stiffness and mass matrices are used to determine natural vibration modes (eigenvectors) and frequencies (eigenvalues). This helps engineers design structures that avoid resonant frequencies and withstand environmental forces.\n",
        "\n",
        "Real-World Examples: Earthquake-proof buildings, aerospace engineering for stability of aircraft components, and designing car suspensions.\n",
        "\n",
        "4. Quantum Mechanics and Quantum Computing\n",
        "\n",
        "Application: Solving quantum systems and computing eigenstates of operators.\n",
        "\n",
        "How Eigen Decomposition is Used: In quantum mechanics, observable properties (like energy) correspond to eigenvalues of certain operators (such as the Hamiltonian). The eigenvectors represent possible quantum states with specific energy levels. Eigen decomposition helps find the stationary states of these systems.\n",
        "\n",
        "Real-World Examples: Quantum chemistry for modeling molecular energy states, design of quantum circuits, and algorithms in quantum computing.\n",
        "\n",
        "5. Image Processing and Facial Recognition\n",
        "\n",
        "Application: Image compression, enhancement, and recognition.\n",
        "\n",
        "How Eigen Decomposition is Used: Techniques like PCA and Singular Value Decomposition (SVD) use eigen decomposition for image feature extraction. Eigenfaces, for example, are computed from the covariance matrix of face images, allowing facial recognition systems to represent faces in a lower-dimensional space.\n",
        "\n",
        "Real-World Examples: Security systems with facial recognition, photo organization in social media, and medical imaging analysis."
      ],
      "metadata": {
        "id": "u2_KQVvxvMw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?**\n"
      ],
      "metadata": {
        "id": "5MJ1PhIqo2Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, a matrix can have multiple sets of eigenvectors and eigenvalues, but with certain conditions. Here‚Äôs a breakdown of when and why this occurs:\n",
        "\n",
        "1. Distinct Eigenvalues\n",
        "\n",
        "If a matrix has distinct eigenvalues (each eigenvalue is unique), it will have a unique set of eigenvectors associated with each eigenvalue.\n",
        "\n",
        "For example, a 2x2 matrix with two different eigenvalues will have two unique eigenvectors, one for each eigenvalue.\n",
        "\n",
        "2. Repeated Eigenvalues\n",
        "\n",
        "If a matrix has repeated (or degenerate) eigenvalues, it may have multiple linearly independent eigenvectors for each repeated eigenvalue. In this case, the matrix is said to be defective if it lacks a complete set of linearly independent eigenvectors.\n",
        "\n",
        "For example, a matrix with an eigenvalue that appears twice could, in theory, have either one or two linearly independent eigenvectors associated with it.\n",
        "\n",
        "3. Symmetric Matrices (or Hermitian for complex values)\n",
        "\n",
        "Symmetric (or Hermitian) matrices always have a complete set of linearly independent eigenvectors regardless of whether the eigenvalues are distinct or repeated. This is a result of the spectral theorem.\n",
        "\n",
        "For symmetric matrices, even repeated eigenvalues will still yield enough linearly independent eigenvectors to form a complete basis for the space.\n",
        "\n",
        "4. Non-symmetric or Non-diagonalizable Matrices\n",
        "\n",
        "Non-symmetric matrices may not have enough linearly independent eigenvectors to form a basis, especially if they have repeated eigenvalues. Such matrices are not diagonalizable and are referred to as defective.\n",
        "\n",
        "For these matrices, the eigenvectors associated with a repeated eigenvalue may span a subspace of lower dimensionality than expected."
      ],
      "metadata": {
        "id": "89-emh4bvjMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.**"
      ],
      "metadata": {
        "id": "DMQSe3czo2J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigen-decomposition is a powerful tool in data analysis and machine learning, providing a foundation for various techniques used in dimensionality reduction, pattern recognition, and data compression. Here are three specific applications or techniques in which eigen-decomposition is essential:\n",
        "\n",
        "\n",
        "1. Principal Component Analysis (PCA)\n",
        "\n",
        "Purpose: PCA is used for dimensionality reduction, noise reduction, and feature extraction. It helps in transforming high-dimensional data into a lower-dimensional space while retaining the most significant patterns.\n",
        "\n",
        "How Eigen-Decomposition is Used: PCA involves finding the eigenvalues and eigenvectors of the covariance matrix of the data. The eigenvectors, called principal components, represent directions of maximum variance in the data. The eigenvalues indicate the variance explained by each principal component.\n",
        "\n",
        "Application: In data analysis, PCA can reduce the number of features, simplifying models and reducing computation time while preserving important patterns. It is commonly used in image processing, genetic data analysis, and customer segmentation, where datasets often contain highly correlated features.\n",
        "\n",
        "2. Spectral Clustering\n",
        "\n",
        "Purpose: Spectral clustering is a technique for identifying clusters in data, particularly effective for non-linearly separable data where traditional clustering algorithms like k-means struggle.\n",
        "\n",
        "How Eigen-Decomposition is Used: Spectral clustering leverages the eigenvalues and eigenvectors of a similarity matrix (e.g., a Laplacian matrix) derived from the data. By decomposing this matrix, the algorithm finds a lower-dimensional representation of the data that captures connectivity between points. Then, it applies a clustering algorithm like k-means on this transformed space.\n",
        "\n",
        "Application: Spectral clustering is used in community detection in social networks, image segmentation, and natural language processing. For instance, in image segmentation, spectral clustering can separate objects in an image based on the similarity of pixels, regardless of complex boundaries.\n",
        "\n",
        "3. Latent Semantic Analysis (LSA) in Natural Language Processing (NLP)\n",
        "\n",
        "Purpose: LSA is used in text analysis to uncover hidden relationships between words and documents. It reduces the dimensionality of text data by identifying semantic structures and patterns in large text corpora.\n",
        "\n",
        "How Eigen-Decomposition is Used: LSA applies eigen-decomposition (specifically, Singular Value Decomposition, or SVD, which is closely related) to the term-document matrix. This decomposition identifies latent concepts by projecting both words and documents into a lower-dimensional semantic space, where words with similar meanings have similar vector representations.\n",
        "\n",
        "Application: LSA is widely used in recommendation systems, information retrieval, and document clustering. For example, search engines use LSA to match search queries with relevant documents by capturing semantic similarities, even if the exact words do not match."
      ],
      "metadata": {
        "id": "JCBGbn3wvwge"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIHLc2ANodHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}